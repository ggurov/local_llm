# Local LLM Stack Configuration
# Optimized for RTX 4090 (24GB) + 128GB RAM + AMD processor

# HuggingFace Configuration
HF_HOME=/home/ggurov/local_llm/data/hf_cache
HF_TOKEN=

# Service URLs
OPENAI_COMPAT_URL=http://localhost:8000/v1
EMBED_URL=http://localhost:8081
QDRANT_URL=http://localhost:6333
LANGFUSE_URL=http://localhost:3000

# GPU Configuration (RTX 4090 as primary, no secondary GPU for now)
NVIDIA_VISIBLE_DEVICES_LLM=0
NVIDIA_VISIBLE_DEVICES_EMB=0

# Model Configuration
MODEL_ID=Qwen/Qwen2.5-7B-Instruct-AWQ
EMBEDDINGS_MODEL=BAAI/bge-large-en-v1.5

# vLLM Configuration (optimized for 24GB VRAM)
MAX_MODEL_LEN=8192
GPU_MEM_UTIL=0.80
BATCH_SIZE=1

# Orchestrator Configuration
ORCHESTRATOR_PORT=8001
LOG_LEVEL=INFO

# Security
NEXTAUTH_SECRET=devsecret_change_in_production
